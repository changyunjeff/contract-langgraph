<style>
    body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;
        line-height: 1.8;
        color: #333;
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }
    h1 {
        color: #667eea;
        border-bottom: 3px solid #667eea;
        padding-bottom: 10px;
    }
    h2 {
        color: #667eea;
        margin-top: 40px;
        border-bottom: 2px solid #667eea;
        padding-bottom: 8px;
    }
    h3 {
        color: #764ba2;
        margin-top: 30px;
    }
    .feature-box {
        background: #f8f9fa;
        border-left: 4px solid #667eea;
        padding: 20px;
        margin: 20px 0;
        border-radius: 4px;
    }
    .warning {
        background: #fff3cd;
        border-left: 4px solid #ffc107;
        padding: 15px;
        margin: 20px 0;
        border-radius: 4px;
    }
    .info {
        background: #d1ecf1;
        border-left: 4px solid #17a2b8;
        padding: 15px;
        margin: 20px 0;
        border-radius: 4px;
    }
    .success {
        background: #d4edda;
        border-left: 4px solid #28a745;
        padding: 15px;
        margin: 20px 0;
        border-radius: 4px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
    }
    th, td {
        padding: 12px;
        text-align: left;
        border-bottom: 1px solid #ddd;
    }
    th {
        background: #667eea;
        color: white;
        font-weight: bold;
    }
    tr:hover {
        background: #f5f5f5;
    }
    code {
        background: #f4f4f4;
        padding: 2px 6px;
        border-radius: 3px;
        font-family: 'Courier New', monospace;
    }
    pre {
        background: #2d2d2d;
        color: #f8f8f2;
        padding: 20px;
        border-radius: 8px;
        overflow-x: auto;
        margin: 20px 0;
    }
    pre code {
        background: transparent;
        color: #f8f8f2;
        padding: 0;
    }
</style>

# ğŸš€ LLM æœåŠ¡ä½¿ç”¨æŒ‡å—

**é«˜æ•ˆã€æ™ºèƒ½çš„è¯­è¨€æ¨¡å‹æœåŠ¡ç®¡ç†æ¡†æ¶**

---

## ğŸ“– ç®€ä»‹

LLM æœåŠ¡æ¨¡å—æä¾›äº†ä¸€ä¸ªé«˜çº§æ¥å£ï¼Œç”¨äºç®¡ç†å’Œä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ¨¡å—é‡‡ç”¨å·¥å‚æ¨¡å¼å’Œå•ä¾‹ç®¡ç†å™¨æ¨¡å¼ï¼Œå®ç°äº† LLM å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€æ™ºèƒ½ç¼“å­˜å’Œè‡ªåŠ¨èµ„æºæ¸…ç†ã€‚

<div class="feature-box">

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **è‡ªåŠ¨ç¼“å­˜ç®¡ç†**ï¼šç›¸åŒé…ç½®çš„ LLM å¯¹è±¡ä¼šè¢«è‡ªåŠ¨ç¼“å­˜å’Œå¤ç”¨
- **èµ„æºè‡ªåŠ¨æ¸…ç†**ï¼šæ”¯æŒ `with` è¯­å¥è‡ªåŠ¨é‡Šæ”¾èµ„æº
- **çº¿ç¨‹å®‰å…¨**ï¼šå¤šçº¿ç¨‹ç¯å¢ƒä¸‹å®‰å…¨ä½¿ç”¨
- **ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šè‡ªåŠ¨ç®¡ç† LLM å¯¹è±¡çš„åˆ›å»ºã€ä½¿ç”¨å’Œå›æ”¶
- **å®šæœŸæ¸…ç†**ï¼šåå°çº¿ç¨‹å®šæœŸæ¸…ç†è¿‡æœŸçš„ç¼“å­˜å¯¹è±¡

</div>

---

## ğŸ”§ å®‰è£…ä¸é…ç½®

### ä¾èµ–å®‰è£…

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
# é¡¹ç›®ä¾èµ–å·²åœ¨ pyproject.toml ä¸­å®šä¹‰
pip install -e .
```

### ç¯å¢ƒå˜é‡é…ç½®

<div class="info">

**ğŸ’¡ æç¤ºï¼š** è®¾ç½® `OPENAI_API_KEY` ç¯å¢ƒå˜é‡ï¼Œæˆ–åœ¨ä½¿ç”¨æ—¶é€šè¿‡é…ç½®ä¼ å…¥ API keyã€‚

</div>

```bash
# åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®
OPENAI_API_KEY=your-api-key-here
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
from src.llm import create_service

# åˆ›å»ºæœåŠ¡å®ä¾‹ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
service = create_service()

# è°ƒç”¨ LLM
response = service.invoke("Hello, world!")
print(response)

# æ‰‹åŠ¨é‡Šæ”¾èµ„æº
service.release()
```

### ä½¿ç”¨ with è¯­å¥ï¼ˆæ¨èï¼‰

<div class="success">

**âœ… æœ€ä½³å®è·µï¼š** ä½¿ç”¨ `with` è¯­å¥å¯ä»¥ç¡®ä¿èµ„æºè‡ªåŠ¨é‡Šæ”¾ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨ `release()`ã€‚

</div>

```python
from src.llm import create_service

# ä½¿ç”¨ with è¯­å¥è‡ªåŠ¨ç®¡ç†èµ„æº
with create_service() as service:
    response = service.invoke("What is Python?")
    print(response)
# é€€å‡º with å—æ—¶è‡ªåŠ¨é‡Šæ”¾èµ„æº
```

### è‡ªå®šä¹‰é…ç½®

```python
from src.llm import create_service

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„æœåŠ¡
with create_service({
    "model_name": "gpt-4",
    "temperature": 0.5,
    "max_tokens": 1000
}) as service:
    response = service.invoke("Explain machine learning")
    print(response)
```

---

## ğŸ“š API å‚è€ƒ

### create_service()

åˆ›å»º LLM æœåŠ¡å®ä¾‹çš„ä¾¿æ·å‡½æ•°ã€‚

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `config` | `Dict[str, Any] \| None` | é…ç½®å­—å…¸ | `None` |

### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `model_name` | `str` | æ¨¡å‹åç§° | `"gpt-3.5-turbo"` |
| `temperature` | `float` | é‡‡æ ·æ¸©åº¦ï¼ˆ0-2ï¼‰ | `0.7` |
| `max_tokens` | `int \| None` | æœ€å¤§ç”Ÿæˆ token æ•° | `None` |
| `api_key` | `str \| None` | OpenAI API å¯†é’¥ | ä»ç¯å¢ƒå˜é‡è¯»å– |
| `base_url` | `str \| None` | API åŸºç¡€ URL | OpenAI é»˜è®¤ URL |

### Service æ–¹æ³•

#### invoke(prompt: str, **kwargs) -> str

è°ƒç”¨ LLM ç”Ÿæˆå“åº”ã€‚

```python
response = service.invoke("Your prompt here")
```

#### batch_invoke(prompts: List[str], **kwargs) -> List[str]

æ‰¹é‡è°ƒç”¨ LLMã€‚

```python
responses = service.batch_invoke([
    "Prompt 1",
    "Prompt 2",
    "Prompt 3"
])
```

#### stream(prompt: str, **kwargs) -> Generator

æµå¼ç”Ÿæˆå“åº”ã€‚

```python
for chunk in service.stream("Your prompt"):
    print(chunk, end="")
```

#### release()

æ‰‹åŠ¨é‡Šæ”¾ LLM èµ„æºï¼ˆé€šå¸¸åœ¨ `with` è¯­å¥ä¸­è‡ªåŠ¨è°ƒç”¨ï¼‰ã€‚

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šç®€å•å¯¹è¯

```python
from src.llm import create_service

with create_service() as service:
    question = "What is the capital of France?"
    answer = service.invoke(question)
    print(f"Q: {question}")
    print(f"A: {answer}")
```

### ç¤ºä¾‹ 2ï¼šæ‰¹é‡å¤„ç†

```python
from src.llm import create_service

questions = [
    "What is Python?",
    "What is machine learning?",
    "What is deep learning?"
]

with create_service() as service:
    answers = service.batch_invoke(questions)
    for q, a in zip(questions, answers):
        print(f"Q: {q}\nA: {a}\n")
```

### ç¤ºä¾‹ 3ï¼šæµå¼è¾“å‡º

```python
from src.llm import create_service

with create_service() as service:
    print("Response: ", end="")
    for chunk in service.stream("Tell me a story"):
        if hasattr(chunk, 'content'):
            print(chunk.content, end="", flush=True)
        else:
            print(chunk, end="", flush=True)
    print()  # æ¢è¡Œ
```

### ç¤ºä¾‹ 4ï¼šä½¿ç”¨ GPT-4

```python
from src.llm import create_service

with create_service({
    "model_name": "gpt-4",
    "temperature": 0.3
}) as service:
    response = service.invoke("Write a Python function to calculate factorial")
    print(response)
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

<div class="warning">

**âš ï¸ é‡è¦æç¤ºï¼š**

- å§‹ç»ˆä½¿ç”¨ `with` è¯­å¥æˆ–æ‰‹åŠ¨è°ƒç”¨ `release()` æ¥é‡Šæ”¾èµ„æº
- ç›¸åŒé…ç½®çš„ LLM å¯¹è±¡ä¼šè¢«ç¼“å­˜å’Œå¤ç”¨ï¼Œæé«˜æ•ˆç‡
- ç¡®ä¿è®¾ç½®äº†æ­£ç¡®çš„ API keyï¼Œå¦åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸
- åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­ï¼ŒæœåŠ¡æ˜¯çº¿ç¨‹å®‰å…¨çš„

</div>

<div class="info">

**â„¹ï¸ æ€§èƒ½ä¼˜åŒ–ï¼š**

- ç®¡ç†å™¨ä¼šè‡ªåŠ¨ç¼“å­˜æœªä½¿ç”¨çš„ LLM å¯¹è±¡ï¼ˆé»˜è®¤ TTL: 1 å°æ—¶ï¼‰
- åå°çº¿ç¨‹ä¼šå®šæœŸæ¸…ç†è¿‡æœŸçš„ç¼“å­˜å¯¹è±¡
- ç¼“å­˜æ± å¤§å°é™åˆ¶ä¸º 100 ä¸ªå¯¹è±¡ï¼ˆå¯é…ç½®ï¼‰

</div>

---

## ğŸ—ï¸ æ¶æ„è¯´æ˜

LLM æœåŠ¡æ¨¡å—é‡‡ç”¨ä¸‰å±‚æ¶æ„è®¾è®¡ï¼š

1. **Factory å±‚**ï¼šè´Ÿè´£åˆ›å»º LLM å¯¹è±¡å’ŒæœåŠ¡å®ä¾‹
2. **Manager å±‚**ï¼šå…¨å±€å•ä¾‹ç®¡ç†å™¨ï¼Œè´Ÿè´£ LLM å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€ç¼“å­˜å’Œæ¸…ç†
3. **Service å±‚**ï¼šå¯¹å¤–æš´éœ²çš„é«˜çº§æ¥å£ï¼Œæä¾›ä¾¿æ·çš„ LLM è°ƒç”¨æ–¹æ³•

<div class="feature-box">

### å·¥ä½œæµç¨‹

1. Factory åˆ›å»º LLM å¯¹è±¡åï¼Œå‘ Manager æ³¨å†Œ
2. Manager æ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœå­˜åœ¨ç›¸åŒé…ç½®çš„ LLMï¼Œåˆ™å¤ç”¨
3. Service ä» Manager è·å– LLM å¯¹è±¡ï¼ˆå¢åŠ å¼•ç”¨è®¡æ•°ï¼‰
4. ä½¿ç”¨å®Œæ¯•åï¼ŒService é‡Šæ”¾ LLMï¼ˆå‡å°‘å¼•ç”¨è®¡æ•°ï¼‰
5. å½“å¼•ç”¨è®¡æ•°ä¸º 0 æ—¶ï¼ŒLLM è¢«ç§»å…¥ç¼“å­˜æ± 
6. åå°çº¿ç¨‹å®šæœŸæ¸…ç†è¿‡æœŸçš„ç¼“å­˜å¯¹è±¡

</div>

---

**Â© 2024 LLM Service Module | ä½¿ç”¨ LangChain OpenAI æ„å»º**
