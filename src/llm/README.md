# ğŸš€ LLM æœåŠ¡ä½¿ç”¨æŒ‡å—

**é«˜æ•ˆã€æ™ºèƒ½çš„è¯­è¨€æ¨¡å‹æœåŠ¡ç®¡ç†æ¡†æ¶**

---

## ğŸ“– ç®€ä»‹

LLM æœåŠ¡æ¨¡å—æä¾›äº†ä¸€ä¸ªé«˜çº§æ¥å£ï¼Œç”¨äºç®¡ç†å’Œä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ¨¡å—é‡‡ç”¨å·¥å‚æ¨¡å¼å’Œå•ä¾‹ç®¡ç†å™¨æ¨¡å¼ï¼Œå®ç°äº† LLM å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€æ™ºèƒ½ç¼“å­˜å’Œè‡ªåŠ¨èµ„æºæ¸…ç†ã€‚

<div class="feature-box">

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- **è‡ªåŠ¨ç¼“å­˜ç®¡ç†**ï¼šç›¸åŒé…ç½®çš„ LLM å¯¹è±¡ä¼šè¢«è‡ªåŠ¨ç¼“å­˜å’Œå¤ç”¨
- **èµ„æºè‡ªåŠ¨æ¸…ç†**ï¼šæ”¯æŒ `with` è¯­å¥è‡ªåŠ¨é‡Šæ”¾èµ„æº
- **çº¿ç¨‹å®‰å…¨**ï¼šå¤šçº¿ç¨‹ç¯å¢ƒä¸‹å®‰å…¨ä½¿ç”¨
- **ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šè‡ªåŠ¨ç®¡ç† LLM å¯¹è±¡çš„åˆ›å»ºã€ä½¿ç”¨å’Œå›æ”¶
- **å®šæœŸæ¸…ç†**ï¼šåå°çº¿ç¨‹å®šæœŸæ¸…ç†è¿‡æœŸçš„ç¼“å­˜å¯¹è±¡

</div>

---

## ğŸ”§ å®‰è£…ä¸é…ç½®

### ä¾èµ–å®‰è£…

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
# é¡¹ç›®ä¾èµ–å·²åœ¨ pyproject.toml ä¸­å®šä¹‰
pip install -e .
```

### ç¯å¢ƒå˜é‡é…ç½®

<div class="info">

**ğŸ’¡ æç¤ºï¼š** è®¾ç½® `OPENAI_API_KEY` ç¯å¢ƒå˜é‡ï¼Œæˆ–åœ¨ä½¿ç”¨æ—¶é€šè¿‡é…ç½®ä¼ å…¥ API keyã€‚

</div>

```bash
# åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®
OPENAI_API_KEY=your-api-key-here
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
from src.llm import create_service

# åˆ›å»ºæœåŠ¡å®ä¾‹ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
service = create_service()

# è°ƒç”¨ LLM
response = service.invoke("Hello, world!")
print(response)

# æ‰‹åŠ¨é‡Šæ”¾èµ„æº
service.release()
```

### ä½¿ç”¨ with è¯­å¥ï¼ˆæ¨èï¼‰

<div class="success">

**âœ… æœ€ä½³å®è·µï¼š** ä½¿ç”¨ `with` è¯­å¥å¯ä»¥ç¡®ä¿èµ„æºè‡ªåŠ¨é‡Šæ”¾ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨ `release()`ã€‚

</div>

```python
from src.llm import create_service

# ä½¿ç”¨ with è¯­å¥è‡ªåŠ¨ç®¡ç†èµ„æº
with create_service() as service:
    response = service.invoke("What is Python?")
    print(response)
# é€€å‡º with å—æ—¶è‡ªåŠ¨é‡Šæ”¾èµ„æº
```

### è‡ªå®šä¹‰é…ç½®

```python
from src.llm import create_service

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„æœåŠ¡
with create_service({
    "model_name": "gpt-4",
    "temperature": 0.5,
    "max_tokens": 1000
}) as service:
    response = service.invoke("Explain machine learning")
    print(response)
```

---

## ğŸ“š API å‚è€ƒ

### create_service()

åˆ›å»º LLM æœåŠ¡å®ä¾‹çš„ä¾¿æ·å‡½æ•°ã€‚

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `config` | `Dict[str, Any] \| None` | é…ç½®å­—å…¸ | `None` |

### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `model_name` | `str` | æ¨¡å‹åç§° | `"gpt-3.5-turbo"` |
| `temperature` | `float` | é‡‡æ ·æ¸©åº¦ï¼ˆ0-2ï¼‰ | `0.7` |
| `max_tokens` | `int \| None` | æœ€å¤§ç”Ÿæˆ token æ•° | `None` |
| `api_key` | `str \| None` | OpenAI API å¯†é’¥ | ä»ç¯å¢ƒå˜é‡è¯»å– |
| `base_url` | `str \| None` | API åŸºç¡€ URL | OpenAI é»˜è®¤ URL |

### Service æ–¹æ³•

#### invoke(prompt: str, **kwargs) -> str

è°ƒç”¨ LLM ç”Ÿæˆå“åº”ã€‚

```python
response = service.invoke("Your prompt here")
```

#### batch_invoke(prompts: List[str], **kwargs) -> List[str]

æ‰¹é‡è°ƒç”¨ LLMã€‚

```python
responses = service.batch_invoke([
    "Prompt 1",
    "Prompt 2",
    "Prompt 3"
])
```

#### stream(prompt: str, **kwargs) -> Generator

æµå¼ç”Ÿæˆå“åº”ã€‚

```python
for chunk in service.stream("Your prompt"):
    print(chunk, end="")
```

#### release()

æ‰‹åŠ¨é‡Šæ”¾ LLM èµ„æºï¼ˆé€šå¸¸åœ¨ `with` è¯­å¥ä¸­è‡ªåŠ¨è°ƒç”¨ï¼‰ã€‚

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šç®€å•å¯¹è¯

```python
from src.llm import create_service

with create_service() as service:
    question = "What is the capital of France?"
    answer = service.invoke(question)
    print(f"Q: {question}")
    print(f"A: {answer}")
```

### ç¤ºä¾‹ 2ï¼šæ‰¹é‡å¤„ç†

```python
from src.llm import create_service

questions = [
    "What is Python?",
    "What is machine learning?",
    "What is deep learning?"
]

with create_service() as service:
    answers = service.batch_invoke(questions)
    for q, a in zip(questions, answers):
        print(f"Q: {q}\nA: {a}\n")
```

### ç¤ºä¾‹ 3ï¼šæµå¼è¾“å‡º

```python
from src.llm import create_service

with create_service() as service:
    print("Response: ", end="")
    for chunk in service.stream("Tell me a story"):
        if hasattr(chunk, 'content'):
            print(chunk.content, end="", flush=True)
        else:
            print(chunk, end="", flush=True)
    print()  # æ¢è¡Œ
```

### ç¤ºä¾‹ 4ï¼šä½¿ç”¨ GPT-4

```python
from src.llm import create_service

with create_service({
    "model_name": "gpt-4",
    "temperature": 0.3
}) as service:
    response = service.invoke("Write a Python function to calculate factorial")
    print(response)
```

### ç¤ºä¾‹ 5ï¼šä¸º LLM ç»‘å®šå·¥å…·

```python
from langchain_core.tools import tool
from src.llm import create_service

# å®šä¹‰å·¥å…·
@tool
def add(a: int, b: int) -> int:
    """Adds two numbers together."""
    return a + b

@tool
def multiply(a: int, b: int) -> int:
    """Multiplies two numbers together."""
    return a * b

# åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [add, multiply]

# åˆ›å»ºæœåŠ¡å¹¶è·å– LLM
with create_service() as service:
    llm = service.get_llm()
    
    # ç»‘å®šå·¥å…·åˆ° LLM
    llm_with_tools = llm.bind_tools(tools)
    
    # ä½¿ç”¨å¸¦å·¥å…·çš„ LLM
    response = llm_with_tools.invoke("What is 5 + 3?")
    print(response)
```

content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 52, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_3dcd5944f5', 'id': 'chatcmpl-Clk5dCkIEipVEn8soyDm3E7JWTWHp', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b0fae-0071-7973-816e-f8f39db73e2b-0' tool_calls=[{'name': 'add', 'args': {'a': 10, 'b': 10}, 'id': 'call_qsaH3LV5SUMMEsZc63xMuewE', 'type': 'tool_call'}] usage_metadata={'input_tokens': 52, 'output_tokens': 18, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}


---

## ğŸ”§ å·¥å…·ç»‘å®šä¸ Agent é›†æˆ

### å®šä¹‰å·¥å…·

ä½¿ç”¨ `@tool` è£…é¥°å™¨å®šä¹‰å·¥å…·å‡½æ•°ï¼š

```python
from langchain_core.tools import tool

@tool
def add(a: int, b: int) -> int:
    """Adds two numbers together.
    
    Args:
        a: First number
        b: Second number
    
    Returns:
        Sum of a and b
    """
    return a + b

@tool
def get_weather(city: str) -> str:
    """Get the current weather for a city.
    
    Args:
        city: Name of the city
    
    Returns:
        Weather information
    """
    # å®ç°å¤©æ°”æŸ¥è¯¢é€»è¾‘
    return f"Weather in {city}: Sunny, 25Â°C"
```

### ç»‘å®šå·¥å…·åˆ° LLM

ä»æœåŠ¡è·å– LLM å¯¹è±¡åï¼Œä½¿ç”¨ `bind_tools()` æ–¹æ³•ç»‘å®šå·¥å…·ï¼š

```python
from src.llm import create_service
from langchain_core.tools import tool

# å®šä¹‰å·¥å…·
@tool
def add(a: int, b: int) -> int:
    """Adds two numbers."""
    return a + b

# åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [add]

# åˆ›å»ºæœåŠ¡å¹¶ç»‘å®šå·¥å…·
with create_service() as service:
    # è·å– LLM å¯¹è±¡
    llm = service.get_llm()
    
    # ç»‘å®šå·¥å…·
    llm_with_tools = llm.bind_tools(tools)
    
    # ç°åœ¨å¯ä»¥ä½¿ç”¨å¸¦å·¥å…·çš„ LLM
    response = llm_with_tools.invoke("Calculate 10 + 20")
```

### åœ¨ Agent ä¸­ä½¿ç”¨å·¥å…·

åœ¨ LangGraph Agent ä¸­é›†æˆå¸¦å·¥å…·çš„ LLMï¼š

```python
from dataclasses import dataclass
from typing import Any, Dict

from langchain_core.tools import tool
from langgraph.graph import StateGraph
from langgraph.runtime import Runtime
from typing_extensions import TypedDict

from src.llm import create_service

# å®šä¹‰å·¥å…·
@tool
def add(a: int, b: int) -> int:
    """Adds two numbers."""
    return a + b

@tool
def multiply(a: int, b: int) -> int:
    """Multiplies two numbers."""
    return a * b

# åˆ›å»ºå·¥å…·åˆ—è¡¨å’Œå­—å…¸
tools = [add, multiply]
tools_by_name = {tool.name: tool for tool in tools}

# åˆ›å»º LLM æœåŠ¡
llm_service = create_service()
llm = llm_service.get_llm()

# ç»‘å®šå·¥å…·åˆ° LLM
llm_with_tools = llm.bind_tools(tools)

# å®šä¹‰ Agent State
@dataclass
class State:
    """Agent state."""
    messages: list = None
    # å…¶ä»–çŠ¶æ€å­—æ®µ...

# å®šä¹‰ Agent èŠ‚ç‚¹
async def agent_node(state: State, runtime: Runtime) -> Dict[str, Any]:
    """Agent node that uses LLM with tools."""
    # ä½¿ç”¨å¸¦å·¥å…·çš„ LLM
    response = llm_with_tools.invoke(state.messages[-1].content)
    
    # å¤„ç†å·¥å…·è°ƒç”¨
    if hasattr(response, 'tool_calls') and response.tool_calls:
        for tool_call in response.tool_calls:
            tool_name = tool_call['name']
            tool_args = tool_call['args']
            
            # æ‰§è¡Œå·¥å…·
            if tool_name in tools_by_name:
                tool_result = tools_by_name[tool_name].invoke(tool_args)
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
                # ...
    
    return {"messages": [response]}

# æ„å»ºå›¾
graph = (
    StateGraph(State)
    .add_node("agent", agent_node)
    .add_edge("__start__", "agent")
    .compile()
)
```

### å®Œæ•´ç¤ºä¾‹ï¼šå¸¦å·¥å…·çš„ Agent

<div class="feature-box">

**å®Œæ•´ç¤ºä¾‹ï¼š**

```python
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage
from src.llm import create_service

# 1. å®šä¹‰å·¥å…·
@tool
def calculator(expression: str) -> str:
    """Evaluates a mathematical expression.
    
    Args:
        expression: Mathematical expression as string (e.g., "2 + 2")
    
    Returns:
        Result of the expression
    """
    try:
        result = eval(expression)  # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
        return str(result)
    except Exception as e:
        return f"Error: {str(e)}"

# 2. åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [calculator]

# 3. åˆ›å»ºæœåŠ¡å¹¶ç»‘å®šå·¥å…·
with create_service() as service:
    llm = service.get_llm()
    llm_with_tools = llm.bind_tools(tools)
    
    # 4. ä½¿ç”¨å¸¦å·¥å…·çš„ LLM
    messages = [HumanMessage(content="What is 15 * 8?")]
    response = llm_with_tools.invoke(messages)
    
    # 5. å¤„ç†å·¥å…·è°ƒç”¨
    if hasattr(response, 'tool_calls') and response.tool_calls:
        for tool_call in response.tool_calls:
            tool_name = tool_call['name']
            tool_args = tool_call['args']
            
            # æ‰§è¡Œå·¥å…·
            if tool_name == 'calculator':
                result = calculator.invoke(tool_args)
                print(f"Tool result: {result}")
    
    print(f"LLM response: {response.content}")
```

</div>

### å·¥å…·ç»‘å®šæœ€ä½³å®è·µ

<div class="info">

**ğŸ’¡ æç¤ºï¼š**

1. **å·¥å…·å®šä¹‰**ï¼šç¡®ä¿å·¥å…·å‡½æ•°æœ‰æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼ŒLLM ä¼šä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥å†³å®šä½•æ—¶è°ƒç”¨å·¥å…·
2. **å·¥å…·å‘½å**ï¼šä½¿ç”¨æè¿°æ€§çš„å·¥å…·åç§°ï¼Œå¸®åŠ© LLM ç†è§£å·¥å…·çš„åŠŸèƒ½
3. **å‚æ•°ç±»å‹**ï¼šæ˜ç¡®å®šä¹‰å‚æ•°ç±»å‹ï¼Œæœ‰åŠ©äº LLM æ­£ç¡®è°ƒç”¨å·¥å…·
4. **é”™è¯¯å¤„ç†**ï¼šåœ¨å·¥å…·å‡½æ•°ä¸­æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†
5. **èµ„æºç®¡ç†**ï¼šä½¿ç”¨ `with` è¯­å¥ç¡®ä¿æœåŠ¡èµ„æºæ­£ç¡®é‡Šæ”¾

</div>

<div class="warning">

**âš ï¸ æ³¨æ„äº‹é¡¹ï¼š**

- ç»‘å®šå·¥å…·åï¼ŒLLM å¯èƒ½ä¼šè¿”å›å·¥å…·è°ƒç”¨è¯·æ±‚ï¼Œéœ€è¦æ£€æŸ¥ `tool_calls` å±æ€§
- å·¥å…·æ‰§è¡Œç»“æœåº”è¯¥åé¦ˆç»™ LLMï¼Œä»¥ä¾¿ç”Ÿæˆæœ€ç»ˆå“åº”
- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œé¿å…ä½¿ç”¨ `eval()` ç­‰ä¸å®‰å…¨çš„å‡½æ•°æ‰§è¡Œç”¨æˆ·è¾“å…¥

</div>

---

## âš ï¸ æ³¨æ„äº‹é¡¹

<div class="warning">

**âš ï¸ é‡è¦æç¤ºï¼š**

- å§‹ç»ˆä½¿ç”¨ `with` è¯­å¥æˆ–æ‰‹åŠ¨è°ƒç”¨ `release()` æ¥é‡Šæ”¾èµ„æº
- ç›¸åŒé…ç½®çš„ LLM å¯¹è±¡ä¼šè¢«ç¼“å­˜å’Œå¤ç”¨ï¼Œæé«˜æ•ˆç‡
- ç¡®ä¿è®¾ç½®äº†æ­£ç¡®çš„ API keyï¼Œå¦åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸
- åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­ï¼ŒæœåŠ¡æ˜¯çº¿ç¨‹å®‰å…¨çš„

</div>

<div class="info">

**â„¹ï¸ æ€§èƒ½ä¼˜åŒ–ï¼š**

- ç®¡ç†å™¨ä¼šè‡ªåŠ¨ç¼“å­˜æœªä½¿ç”¨çš„ LLM å¯¹è±¡ï¼ˆé»˜è®¤ TTL: 1 å°æ—¶ï¼‰
- åå°çº¿ç¨‹ä¼šå®šæœŸæ¸…ç†è¿‡æœŸçš„ç¼“å­˜å¯¹è±¡
- ç¼“å­˜æ± å¤§å°é™åˆ¶ä¸º 100 ä¸ªå¯¹è±¡ï¼ˆå¯é…ç½®ï¼‰

</div>

---

## ğŸ—ï¸ æ¶æ„è¯´æ˜

LLM æœåŠ¡æ¨¡å—é‡‡ç”¨ä¸‰å±‚æ¶æ„è®¾è®¡ï¼š

1. **Factory å±‚**ï¼šè´Ÿè´£åˆ›å»º LLM å¯¹è±¡å’ŒæœåŠ¡å®ä¾‹
2. **Manager å±‚**ï¼šå…¨å±€å•ä¾‹ç®¡ç†å™¨ï¼Œè´Ÿè´£ LLM å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€ç¼“å­˜å’Œæ¸…ç†
3. **Service å±‚**ï¼šå¯¹å¤–æš´éœ²çš„é«˜çº§æ¥å£ï¼Œæä¾›ä¾¿æ·çš„ LLM è°ƒç”¨æ–¹æ³•

<div class="feature-box">

### å·¥ä½œæµç¨‹

1. Factory åˆ›å»º LLM å¯¹è±¡åï¼Œå‘ Manager æ³¨å†Œ
2. Manager æ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœå­˜åœ¨ç›¸åŒé…ç½®çš„ LLMï¼Œåˆ™å¤ç”¨
3. Service ä» Manager è·å– LLM å¯¹è±¡ï¼ˆå¢åŠ å¼•ç”¨è®¡æ•°ï¼‰
4. ä½¿ç”¨å®Œæ¯•åï¼ŒService é‡Šæ”¾ LLMï¼ˆå‡å°‘å¼•ç”¨è®¡æ•°ï¼‰
5. å½“å¼•ç”¨è®¡æ•°ä¸º 0 æ—¶ï¼ŒLLM è¢«ç§»å…¥ç¼“å­˜æ± 
6. åå°çº¿ç¨‹å®šæœŸæ¸…ç†è¿‡æœŸçš„ç¼“å­˜å¯¹è±¡

</div>

---

**Â© 2024 LLM Service Module | ä½¿ç”¨ LangChain OpenAI æ„å»º**
